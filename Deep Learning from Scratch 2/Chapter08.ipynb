{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cecf1e69",
   "metadata": {},
   "source": [
    "# 8.1 어텐션의 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1485e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5, 4)\n",
      "(4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5,4\n",
    "hs = np.random.randn(T,H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis = 1)\n",
    "print(ar.shape)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis = 0)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b188e07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N,T,H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N,T)\n",
    "ar = a.reshape(N,T,1).repeat(H, axis = 2)\n",
    "\n",
    "t = hs * ar\n",
    "print(t.shape)\n",
    "\n",
    "c = np.sum(t, axis = 1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f2cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "083da203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "h = np.random.randn(N,H)\n",
    "hr = h.reshape(N, 1, H)\n",
    "\n",
    "t = hs * hr\n",
    "print(t.shape)\n",
    "\n",
    "s = np.sum(t, axis = 2)\n",
    "print(s.shape)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec4d875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82494e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48eb9da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bfd63b",
   "metadata": {},
   "source": [
    "# 8.2 어텐션을 갖춘 seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09831f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    \n",
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d98c2",
   "metadata": {},
   "source": [
    "# 8.3 어텐션 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb9c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 10[s] | loss 2.97\n",
      "| epoch 1 |  iter 41 / 351 | time 19[s] | loss 1.87\n",
      "| epoch 1 |  iter 61 / 351 | time 30[s] | loss 1.70\n",
      "| epoch 1 |  iter 81 / 351 | time 44[s] | loss 1.47\n",
      "| epoch 1 |  iter 101 / 351 | time 60[s] | loss 1.21\n",
      "| epoch 1 |  iter 121 / 351 | time 78[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 95[s] | loss 1.08\n",
      "| epoch 1 |  iter 161 / 351 | time 113[s] | loss 1.05\n",
      "| epoch 1 |  iter 181 / 351 | time 130[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 146[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 164[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 182[s] | loss 1.01\n",
      "| epoch 1 |  iter 261 / 351 | time 201[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 219[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 237[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 254[s] | loss 0.99\n",
      "| epoch 1 |  iter 341 / 351 | time 271[s] | loss 0.99\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "X 1999-02-21\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "X 1999-02-21\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "X 1999-02-21\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "X 1999-02-21\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "X 1999-02-21\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "X 1999-02-21\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "X 1999-02-21\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "X 1999-02-21\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "X 1999-02-21\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "X 1999-02-21\n",
      "---\n",
      "val acc 0.020%\n",
      "| epoch 2 |  iter 1 / 351 | time 1[s] | loss 0.99\n",
      "| epoch 2 |  iter 21 / 351 | time 20[s] | loss 0.99\n",
      "| epoch 2 |  iter 41 / 351 | time 40[s] | loss 0.98\n",
      "| epoch 2 |  iter 61 / 351 | time 60[s] | loss 0.97\n",
      "| epoch 2 |  iter 81 / 351 | time 79[s] | loss 0.96\n",
      "| epoch 2 |  iter 101 / 351 | time 97[s] | loss 0.93\n",
      "| epoch 2 |  iter 121 / 351 | time 114[s] | loss 0.90\n",
      "| epoch 2 |  iter 141 / 351 | time 132[s] | loss 0.86\n",
      "| epoch 2 |  iter 161 / 351 | time 150[s] | loss 0.82\n",
      "| epoch 2 |  iter 181 / 351 | time 167[s] | loss 0.74\n",
      "| epoch 2 |  iter 201 / 351 | time 185[s] | loss 0.64\n",
      "| epoch 2 |  iter 221 / 351 | time 203[s] | loss 0.53\n",
      "| epoch 2 |  iter 241 / 351 | time 222[s] | loss 0.41\n",
      "| epoch 2 |  iter 261 / 351 | time 239[s] | loss 0.30\n",
      "| epoch 2 |  iter 281 / 351 | time 258[s] | loss 0.21\n",
      "| epoch 2 |  iter 301 / 351 | time 276[s] | loss 0.14\n",
      "| epoch 2 |  iter 321 / 351 | time 293[s] | loss 0.10\n",
      "| epoch 2 |  iter 341 / 351 | time 312[s] | loss 0.07\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 98.200%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.04\n",
      "| epoch 3 |  iter 21 / 351 | time 19[s] | loss 0.04\n",
      "| epoch 3 |  iter 41 / 351 | time 36[s] | loss 0.03\n",
      "| epoch 3 |  iter 61 / 351 | time 55[s] | loss 0.03\n",
      "| epoch 3 |  iter 81 / 351 | time 74[s] | loss 0.02\n",
      "| epoch 3 |  iter 101 / 351 | time 93[s] | loss 0.02\n",
      "| epoch 3 |  iter 121 / 351 | time 112[s] | loss 0.01\n",
      "| epoch 3 |  iter 141 / 351 | time 131[s] | loss 0.01\n",
      "| epoch 3 |  iter 161 / 351 | time 150[s] | loss 0.01\n",
      "| epoch 3 |  iter 181 / 351 | time 167[s] | loss 0.01\n",
      "| epoch 3 |  iter 201 / 351 | time 185[s] | loss 0.01\n",
      "| epoch 3 |  iter 221 / 351 | time 204[s] | loss 0.01\n",
      "| epoch 3 |  iter 241 / 351 | time 223[s] | loss 0.01\n",
      "| epoch 3 |  iter 261 / 351 | time 241[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 258[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 276[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 295[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 314[s] | loss 0.01\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 99.980%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 4 |  iter 41 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 4 |  iter 61 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 4 |  iter 81 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 4 |  iter 101 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 4 |  iter 121 / 351 | time 97[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 113[s] | loss 0.00\n",
      "| epoch 4 |  iter 161 / 351 | time 131[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 147[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 165[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 182[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 199[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 216[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 232[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 249[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 268[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 285[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 99.980%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 34[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 51[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 68[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 85[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 121[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 139[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 156[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 174[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 193[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 211[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 232[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 251[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 269[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 287[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 304[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 19[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 36[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 54[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 72[s] | loss 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 6 |  iter 101 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 109[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 128[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 146[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 164[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 184[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 203[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 221[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 239[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 257[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 276[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 294[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 313[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 21 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 7 |  iter 41 / 351 | time 36[s] | loss 0.00\n",
      "| epoch 7 |  iter 61 / 351 | time 53[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 71[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 89[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 108[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 126[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 145[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 163[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 180[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 198[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 217[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 235[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 254[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 273[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 290[s] | loss 0.00\n",
      "| epoch 7 |  iter 341 / 351 | time 309[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 8 |  iter 1 / 351 | time 1[s] | loss 0.00\n",
      "| epoch 8 |  iter 21 / 351 | time 20[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 57[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 76[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 95[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 113[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 131[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 149[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 168[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 186[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 204[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 223[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 242[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 261[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 281[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 300[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 318[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 56[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 94[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 113[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 133[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 152[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 172[s] | loss 0.05\n",
      "| epoch 9 |  iter 201 / 351 | time 192[s] | loss 0.04\n",
      "| epoch 9 |  iter 221 / 351 | time 211[s] | loss 0.01\n",
      "| epoch 9 |  iter 241 / 351 | time 230[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 249[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 267[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 287[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 306[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 325[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 99.980%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 18[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 36[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 54[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 73[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 108[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 124[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 142[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 158[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 176[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 192[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 210[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 227[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 245[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 262[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 279[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 296[s] | loss 0.00\n",
      "Q WEDNESDAY, AUGUST 6, 1997    \n",
      "T 1997-08-06\n",
      "O 1997-08-06\n",
      "---\n",
      "Q DEC 12, 1972                 \n",
      "T 1972-12-12\n",
      "O 1972-12-12\n",
      "---\n",
      "Q TUESDAY, AUGUST 11, 2009     \n",
      "T 2009-08-11\n",
      "O 2009-08-11\n",
      "---\n",
      "Q TUESDAY, OCTOBER 12, 2010    \n",
      "T 2010-10-12\n",
      "O 2010-10-12\n",
      "---\n",
      "Q JUN 11, 1979                 \n",
      "T 1979-06-11\n",
      "O 1979-06-11\n",
      "---\n",
      "Q 3/9/75                       \n",
      "T 1975-03-09\n",
      "O 1975-03-09\n",
      "---\n",
      "Q Saturday, March 5, 1994      \n",
      "T 1994-03-05\n",
      "O 1994-03-05\n",
      "---\n",
      "Q Jan 22, 1996                 \n",
      "T 1996-01-22\n",
      "O 1996-01-22\n",
      "---\n",
      "Q aug 9, 1989                  \n",
      "T 1989-08-09\n",
      "O 1989-08-09\n",
      "---\n",
      "Q Monday, January 29, 2001     \n",
      "T 2001-01-29\n",
      "O 2001-01-29\n",
      "---\n",
      "val acc 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.time_layers import *\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45570eca",
   "metadata": {},
   "source": [
    "# 8.4 어텐션에 관한 남은 이야기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044982f5",
   "metadata": {},
   "source": [
    "# 8.5 어텐션 응용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
